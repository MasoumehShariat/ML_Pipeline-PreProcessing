{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data transformation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Normalization**\n",
        "\n",
        "Normalization is the process of scaling individual samples to have unit norm. This process can be useful if you plan to use a quadratic form such as the dot-product or any other kernel to quantify the similarity of any pair of samples.\n",
        "\n",
        "This assumption is the base of the Vector Space Model often used in text classification and clustering contexts.\n",
        "\n",
        "The function normalize provides a quick and easy way to perform this operation on a single array-like dataset, either using the L1, L2, or max norms.\n",
        "\n",
        "Normalization makes the features more consistent with each other, which allows the model to predict outputs more accurately.\n"
      ],
      "metadata": {
        "id": "pHc7D5IRhYDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing"
      ],
      "metadata": {
        "id": "XoIYNjY6ijzo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [[ 1., -1.,  2.],  [ 2.,  0.,  0.],  [ 0.,  1., -1.]]\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99ULljCQjLi7",
        "outputId": "28898e8b-da2e-4ed0-a292-6e15dd8f3635"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.0, -1.0, 2.0], [2.0, 0.0, 0.0], [0.0, 1.0, -1.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) L1 Normalization**\n",
        "\n",
        "The L1 norm that is calculated as the sum of the absolute values of the vector.\n"
      ],
      "metadata": {
        "id": "nI74gqCykWBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#L1 \n",
        "normalized = preprocessing.normalize(data, norm='l1')\n",
        "normalized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnSSHy_7h4FZ",
        "outputId": "ee3bd7c3-a475-4a50-995d-beb0418b7248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.25, -0.25,  0.5 ],\n",
              "       [ 1.  ,  0.  ,  0.  ],\n",
              "       [ 0.  ,  0.5 , -0.5 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) L2 Normalization**\n",
        "\n",
        "The L2 norm that is calculated as the square root of the sum of the squared vector values."
      ],
      "metadata": {
        "id": "ik2HwZXjkjpS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#L2 \n",
        "normalized = preprocessing.normalize(data, norm='l2')\n",
        "normalized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzQ73ju3iEX_",
        "outputId": "bfcf29ee-b7de-44d7-d0af-2d91cd7d9702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.40824829, -0.40824829,  0.81649658],\n",
              "       [ 1.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.70710678, -0.70710678]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3) Max Normalization**\n",
        "\n",
        "The max norm that is calculated as the maximum vector values."
      ],
      "metadata": {
        "id": "_XTkP64mksiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#max\n",
        "normalized = preprocessing.normalize(data, norm='max')\n",
        "normalized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AihegAziKZZ",
        "outputId": "7f5542de-ce62-4412-a5dd-c517ef7b1f6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.5, -0.5,  1. ],\n",
              "       [ 1. ,  0. ,  0. ],\n",
              "       [ 0. ,  1. , -1. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Rescaling Data**"
      ],
      "metadata": {
        "id": "FehRiNUq2lpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When our data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale.\n",
        "This is useful for optimization algorithms in used in the core of machine learning algorithms like gradient descent.\n",
        "It is also useful for algorithms that weight inputs like regression and neural networks and algorithms that use distance measures like K-Nearest Neighbors.\n",
        "We can rescale our data using scikit-learn using the MinMaxScaler class.\n"
      ],
      "metadata": {
        "id": "K7wJOGeL28C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "print(scaler.fit(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IelmXT253sYS",
        "outputId": "5476c16e-0525-4ad0-bfc8-8c69dd56d4ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MinMaxScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(scaler.data_max_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31PlfVG_3SCp",
        "outputId": "3c072123-abe3-45e5-f00c-9727417a4334"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 1. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(scaler.transform(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZHoEyGx4FhC",
        "outputId": "860506c1-da94-4218-9786-22a947187326"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.5        0.         1.        ]\n",
            " [1.         0.5        0.33333333]\n",
            " [0.         1.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oo7vuOUu4GyR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}